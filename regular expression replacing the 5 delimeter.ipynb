{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "838c087d-fc96-4caa-b4d2-64ebb8f6766c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------+\n|_c0                                                                                                                  |\n+---------------------------------------------------------------------------------------------------------------------+\n|azar|BE|8|bigdata|8148715811|ramesh|BE|9|hadoop|8148715812|suresh|BE|10|hive|8148715809|dinesh|BE|11|spark|8156715811|\n+---------------------------------------------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(\"/FileStore/tables/regularExpression.txt\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1211666e-f8cd-4a79-868c-d92355ecdb66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------+\n|values                                                                                                                  |\n+------------------------------------------------------------------------------------------------------------------------+\n|azar|BE|8|bigdata|8148715811|-ramesh|BE|9|hadoop|8148715812|-suresh|BE|10|hive|8148715809|-dinesh|BE|11|spark|8156715811|\n+------------------------------------------------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "# Assuming df is your DataFrame containing the column \"_c0\"\n",
    "df_expr = df.withColumn(\"values\", regexp_replace(col(\"_c0\"), \"(.*?\\\\|){5}\",  \"$0-\"))\n",
    "df_drop=df_expr.drop(col(\"_c0\"))\n",
    "df_drop.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bc6bd83-61cc-4f7c-8335-361cdf83e9f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-4069259556708744>:3\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    df_explode.show(truncate=False)\u001B[0m\n",
       "\u001B[0m    ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;36m  File \u001B[0;32m<command-4069259556708744>:3\u001B[0;36m\u001B[0m\n\u001B[0;31m    df_explode.show(truncate=False)\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n",
       "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (<command-4069259556708744>, line 3)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_split=df_drop.select(split(col(\"values\"),\"\\\\|-\").alias(\"split\"))\n",
    "df_explode=df_split.select(explode(split((col(\"split\"),\",\")))\n",
    "df_explode.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df00eee0-3283-4d3e-84f5-64b753d483c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------+\n|split                                                                                                                     |\n+--------------------------------------------------------------------------------------------------------------------------+\n|[azar|BE|8|bigdata|8148715811, ramesh|BE|9|hadoop|8148715812, suresh|BE|10|hive|8148715809, dinesh|BE|11|spark|8156715811]|\n+--------------------------------------------------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_split = df_drop.select(split(col(\"values\"), \"\\\\|-\").alias(\"split\"))\n",
    "df_explode = df_split.select(explode(col(\"split\")).alias(\"exploded_values\"))\n",
    "df_split.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "regular expression replacing the 5 delimeter",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
