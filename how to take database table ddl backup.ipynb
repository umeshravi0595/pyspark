{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d971d124-c2f2-4714-b3b8-9eed8b347ab0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>databaseName</th></tr></thead><tbody><tr><td>default</td></tr><tr><td>test_database</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "default"
        ],
        [
         "test_database"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "databaseName",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "show databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93ef2441-78e4-4439-80dd-0995a16366bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "# Define schemas\n",
    "schema1 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "schema2 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"description\", StringType(), True)\n",
    "])\n",
    "\n",
    "schema3 = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"price\", FloatType(), True)\n",
    "])\n",
    "\n",
    "schema4 = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data1 = [(1, \"Alice\"), (2, \"Bob\")]\n",
    "data2 = [(1, \"First record\"), (2, \"Second record\")]\n",
    "data3 = [(1, 19.99), (2, 29.99)]\n",
    "data4 = [(1, 5), (2, 10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0cb6d6b-ff91-49fc-b3aa-01bed21840b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create DataFrames\n",
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "df2 = spark.createDataFrame(data2, schema2)\n",
    "df3 = spark.createDataFrame(data3, schema3)\n",
    "df4 = spark.createDataFrame(data4, schema4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df102b0e-b6c3-443e-9f65-9d8532547dd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save DataFrames as tables\n",
    "df1.write.mode(\"overwrite\").saveAsTable(\"default.table1\")\n",
    "df2.write.mode(\"overwrite\").saveAsTable(\"default.table2\")\n",
    "df3.write.mode(\"overwrite\").saveAsTable(\"default.table3\")\n",
    "df4.write.mode(\"overwrite\").saveAsTable(\"default.table4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6446392-d3a4-421d-85b5-04358fcc6d50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a new database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS test_database\")\n",
    "df1.write.mode(\"overwrite\").saveAsTable(\"test_database.table1\")\n",
    "df2.write.mode(\"overwrite\").saveAsTable(\"test_database.table2\")\n",
    "df3.write.mode(\"overwrite\").saveAsTable(\"test_database.table3\")\n",
    "df4.write.mode(\"overwrite\").saveAsTable(\"test_database.table4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f756d97-4b7b-4cc2-82fb-4132a530beff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody><tr><td>test_database</td><td>table1</td><td>false</td></tr><tr><td>test_database</td><td>table2</td><td>false</td></tr><tr><td>test_database</td><td>table3</td><td>false</td></tr><tr><td>test_database</td><td>table4</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "test_database",
         "table1",
         false
        ],
        [
         "test_database",
         "table2",
         false
        ],
        [
         "test_database",
         "table3",
         false
        ],
        [
         "test_database",
         "table4",
         false
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "use test_database;\n",
    "show tables ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "691d3a46-8cbd-4cdb-8cea-dfa29303369d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['default', 'test_database']\n"
     ]
    }
   ],
   "source": [
    "# list comprehension in python\n",
    "all_dbs=[ db.databaseName for db in spark.sql(\"show databases\").collect()]\n",
    "print(all_dbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e52161a7-522a-4075-b92b-019ac5db862f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs mkdirs file:/tmp/ddls/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaaeaeb4-dad3-4212-b807-10ced5c89c66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def createddl(database):\n",
    " all_tables=spark.catalog.listTables('default')\n",
    "#The open function in Python is used to open a file. It allows you to create, read, write, or append to files. Here’s a detailed explanation of the open function\n",
    "#\"w\": This is the mode in which the file is opened. The mode \"w\" stands for \"write\". It allows you to write to the file. If the file already exists, it will be truncated (emptied) before writing.\n",
    " f=open(f\"/tmp/ddls/ddl_{database}\",\"w\")\n",
    " for tables in all_tables:\n",
    "    ddl=spark.sql(f\"show create table {database}.{tables.name};\")\n",
    "    # Write some data to the file\n",
    "    f.write(ddl.first()[0])\n",
    "    f.write(\";\\n\")\n",
    " f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ead0d258-02a5-481f-b231-e14ecb560ecc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">res7: Boolean = true\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">res7: Boolean = true\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "createddl('test_database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f12a38-f0ef-4b74-a174-d2e31954b9ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>file:/tmp/ddls/ddl_test_database</td><td>ddl_test_database</td><td>714</td><td>1720182071642</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "file:/tmp/ddls/ddl_test_database",
         "ddl_test_database",
         714,
         1720182071642
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls file:/tmp/ddls/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef33ecf3-b820-4ee0-8325-38e8a8ac4cf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">CREATE TABLE spark_catalog.test_database.table1 (\n",
       "  id INT,\n",
       "  name STRING)\n",
       "USING delta\n",
       "TBLPROPERTIES (\n",
       "  'delta.minReaderVersion' = '1',\n",
       "  'delta.minWriterVersion' = '2')\n",
       ";\n",
       "CREATE TABLE spark_catalog.test_database.table2 (\n",
       "  id INT,\n",
       "  description STRING)\n",
       "USING delta\n",
       "TBLPROPERTIES (\n",
       "  'delta.minReaderVersion' = '1',\n",
       "  'delta.minWriterVersion' = '2')\n",
       ";\n",
       "CREATE TABLE spark_catalog.test_database.table3 (\n",
       "  product_id INT,\n",
       "  price FLOAT)\n",
       "USING delta\n",
       "TBLPROPERTIES (\n",
       "  'delta.minReaderVersion' = '1',\n",
       "  'delta.minWriterVersion' = '2')\n",
       ";\n",
       "CREATE TABLE spark_catalog.test_database.table4 (\n",
       "  order_id INT,\n",
       "  quantity INT)\n",
       "USING delta\n",
       "TBLPROPERTIES (\n",
       "  'delta.minReaderVersion' = '1',\n",
       "  'delta.minWriterVersion' = '2')\n",
       ";\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">CREATE TABLE spark_catalog.test_database.table1 (\n  id INT,\n  name STRING)\nUSING delta\nTBLPROPERTIES (\n  'delta.minReaderVersion' = '1',\n  'delta.minWriterVersion' = '2')\n;\nCREATE TABLE spark_catalog.test_database.table2 (\n  id INT,\n  description STRING)\nUSING delta\nTBLPROPERTIES (\n  'delta.minReaderVersion' = '1',\n  'delta.minWriterVersion' = '2')\n;\nCREATE TABLE spark_catalog.test_database.table3 (\n  product_id INT,\n  price FLOAT)\nUSING delta\nTBLPROPERTIES (\n  'delta.minReaderVersion' = '1',\n  'delta.minWriterVersion' = '2')\n;\nCREATE TABLE spark_catalog.test_database.table4 (\n  order_id INT,\n  quantity INT)\nUSING delta\nTBLPROPERTIES (\n  'delta.minReaderVersion' = '1',\n  'delta.minWriterVersion' = '2')\n;\n\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs head file:/tmp/ddls/ddl_test_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab091a8f-89e6-4216-846f-19aec29e2a47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[48]: [None, None]"
     ]
    }
   ],
   "source": [
    "alldbs = [db.databaseName for db in spark.sql(\"show databases\").collect()]\n",
    "alldbs\n",
    "from multiprocessing.pool import ThreadPool\n",
    "processes = ThreadPool(4)\n",
    "processes.map(createddl,[database for database in alldbs])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2071025783725051,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "how to take database table ddl backup",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
