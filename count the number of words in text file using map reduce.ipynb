{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "539b03fc-7c02-4360-97f9-36eb39ceb186",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">pyspark--------------------------------\r\n",
       "1.repartition s coalesce\r\n",
       "2.cache vs persist\r\n",
       "3.broadcast variable &amp; sort merge join\r\n",
       "4.adaptive query execution\r\n",
       "5.shuffle partition\r\n",
       "6.partitionby\r\n",
       "7.checkpoint\r\n",
       "8.data skew optimization\r\n",
       "9.job stage task\r\n",
       "10.parralizim\r\n",
       "11.serialize \r\n",
       "12.deserialize\r\n",
       "13.salting   ---- pseudo id \r\n",
       "14.bucketing\r\n",
       "15.skew hint\r\n",
       "16.catalyst optimizer\r\n",
       "17.predicate pushdown projection pushdown\r\n",
       "18.RDD vs DATAframe\r\n",
       "19.RBO vs CBO\r\n",
       "20. different file formats in spark\r\n",
       "21. snappy vs gzip\r\n",
       "22.optimize command\r\n",
       "23.spark streaming\r\n",
       "24.dymanmic pruning\r\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">pyspark--------------------------------\r\n1.repartition s coalesce\r\n2.cache vs persist\r\n3.broadcast variable &amp; sort merge join\r\n4.adaptive query execution\r\n5.shuffle partition\r\n6.partitionby\r\n7.checkpoint\r\n8.data skew optimization\r\n9.job stage task\r\n10.parralizim\r\n11.serialize \r\n12.deserialize\r\n13.salting   ---- pseudo id \r\n14.bucketing\r\n15.skew hint\r\n16.catalyst optimizer\r\n17.predicate pushdown projection pushdown\r\n18.RDD vs DATAframe\r\n19.RBO vs CBO\r\n20. different file formats in spark\r\n21. snappy vs gzip\r\n22.optimize command\r\n23.spark streaming\r\n24.dymanmic pruning\r\n\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs head \"/FileStore/tables/azure_data_engineering_questions.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a9a7ec4-acf0-4fd1-8687-4caaf2a934cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.master(\"local\").appName(\"word_count\").getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e23d32d8-5f6e-4f2f-9f66-128b23977e59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text_file=sc.textFile(\"/FileStore/tables/spark_performance_improvement.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ade87608-9651-4fa3-8b65-5a4f10a37263",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[31]: ['spark performance improvement:',\n '',\n '1. File format:',\n '    parquet +snappy is the best because it is columunar file format which gives lot of efficiency in terms of storage space and while doing processing and along with the snappy compression it is used for large datasets',\n '2. parralelisim:',\n '',\n 'Parallelism in the context of Spark refers to the ability to execute multiple tasks concurrently to process data more efficiently. It allows Spark to distribute computation across multiple nodes in a cluster, enabling faster data processing.',\n '',\n '    spark.sparkContext.defaultParallelism = 4',\n ' ',\n '3. serialisation:',\n '',\n 'spark.config.set(\"spark.serializer\",\"kryo serializer\")']"
     ]
    }
   ],
   "source": [
    "text_file.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52e9f22d-e373-4a2d-b3cf-ab5312a080ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: [('spark performance improvement:', 1),\n ('', 4),\n ('1. File format:', 1),\n ('3. serialisation:', 1),\n ('spark.config.set(\"spark.serializer\"', 1),\n ('\"kryo serializer\")', 1),\n ('    parquet +snappy is the best because it is columunar file format which gives lot of efficiency in terms of storage space and while doing processing and along with the snappy compression it is used for large datasets',\n  1),\n ('2. parralelisim:', 1),\n ('Parallelism in the context of Spark refers to the ability to execute multiple tasks concurrently to process data more efficiently. It allows Spark to distribute computation across multiple nodes in a cluster',\n  1),\n (' enabling faster data processing.', 1),\n ('    spark.sparkContext.defaultParallelism = 4', 1),\n (' ', 1)]"
     ]
    }
   ],
   "source": [
    "text_file_flat=text_file.flatMap(lambda x: x.split(','))\\\n",
    "                        .map(lambda x: (x,1))\\\n",
    "                        .reduceByKey(lambda x,y:x+y)\n",
    "text_file_flat.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07680d0f-779e-4dfa-9e85-f98d4b71bf64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd=text_file.map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2199ec9-34d7-478d-b4e4-2976f037c13a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.text(\"/FileStore/tables/spark_performance_improvement.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e77c58ba-44a1-4b74-9191-e99aafbc8c8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>value</th></tr></thead><tbody><tr><td>spark performance improvement:</td></tr><tr><td></td></tr><tr><td>1. File format:</td></tr><tr><td>    parquet +snappy is the best because it is columunar file format which gives lot of efficiency in terms of storage space and while doing processing and along with the snappy compression it is used for large datasets</td></tr><tr><td>2. parralelisim:</td></tr><tr><td></td></tr><tr><td>Parallelism in the context of Spark refers to the ability to execute multiple tasks concurrently to process data more efficiently. It allows Spark to distribute computation across multiple nodes in a cluster, enabling faster data processing.</td></tr><tr><td></td></tr><tr><td>    spark.sparkContext.defaultParallelism = 4</td></tr><tr><td> </td></tr><tr><td>3. serialisation:</td></tr><tr><td></td></tr><tr><td>spark.config.set(\"spark.serializer\",\"kryo serializer\")</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "spark performance improvement:"
        ],
        [
         ""
        ],
        [
         "1. File format:"
        ],
        [
         "    parquet +snappy is the best because it is columunar file format which gives lot of efficiency in terms of storage space and while doing processing and along with the snappy compression it is used for large datasets"
        ],
        [
         "2. parralelisim:"
        ],
        [
         ""
        ],
        [
         "Parallelism in the context of Spark refers to the ability to execute multiple tasks concurrently to process data more efficiently. It allows Spark to distribute computation across multiple nodes in a cluster, enabling faster data processing."
        ],
        [
         ""
        ],
        [
         "    spark.sparkContext.defaultParallelism = 4"
        ],
        [
         " "
        ],
        [
         "3. serialisation:"
        ],
        [
         ""
        ],
        [
         "spark.config.set(\"spark.serializer\",\"kryo serializer\")"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c32ac5c-b96a-4deb-afe1-b1a6fc80ce28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Split the 'value' column into multiple columns based on a space delimiter\n",
    "df_split = df.select(explode(split(df[\"value\"], \" \")).alias(\"split_values\"))\n",
    "\n",
    "# Show the DataFrame with split values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0015b530-3e67-409d-9a6d-79a344c8c944",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>split_values</th><th>count</th></tr></thead><tbody><tr><td>used</td><td>1</td></tr><tr><td>space</td><td>1</td></tr><tr><td>spark.config.set(\"spark.serializer\",\"kryo</td><td>1</td></tr><tr><td>compression</td><td>1</td></tr><tr><td>=</td><td>1</td></tr><tr><td>spark.sparkContext.defaultParallelism</td><td>1</td></tr><tr><td>because</td><td>1</td></tr><tr><td>format</td><td>1</td></tr><tr><td>process</td><td>1</td></tr><tr><td>+snappy</td><td>1</td></tr><tr><td>context</td><td>1</td></tr><tr><td>more</td><td>1</td></tr><tr><td>3.</td><td>1</td></tr><tr><td>parralelisim:</td><td>1</td></tr><tr><td>along</td><td>1</td></tr><tr><td>for</td><td>1</td></tr><tr><td>refers</td><td>1</td></tr><tr><td>distribute</td><td>1</td></tr><tr><td>in</td><td>3</td></tr><tr><td>with</td><td>1</td></tr><tr><td>terms</td><td>1</td></tr><tr><td>nodes</td><td>1</td></tr><tr><td>serializer\")</td><td>1</td></tr><tr><td>allows</td><td>1</td></tr><tr><td>is</td><td>3</td></tr><tr><td>serialisation:</td><td>1</td></tr><tr><td>File</td><td>1</td></tr><tr><td>it</td><td>2</td></tr><tr><td>data</td><td>2</td></tr><tr><td>processing.</td><td>1</td></tr><tr><td>doing</td><td>1</td></tr><tr><td>It</td><td>1</td></tr><tr><td>enabling</td><td>1</td></tr><tr><td>spark</td><td>1</td></tr><tr><td>while</td><td>1</td></tr><tr><td>file</td><td>1</td></tr><tr><td>computation</td><td>1</td></tr><tr><td>the</td><td>4</td></tr><tr><td>across</td><td>1</td></tr><tr><td>format:</td><td>1</td></tr><tr><td>snappy</td><td>1</td></tr><tr><td>datasets</td><td>1</td></tr><tr><td>and</td><td>2</td></tr><tr><td>lot</td><td>1</td></tr><tr><td>cluster,</td><td>1</td></tr><tr><td>of</td><td>3</td></tr><tr><td>gives</td><td>1</td></tr><tr><td>columunar</td><td>1</td></tr><tr><td>ability</td><td>1</td></tr><tr><td>efficiently.</td><td>1</td></tr><tr><td>Parallelism</td><td>1</td></tr><tr><td>performance</td><td>1</td></tr><tr><td>tasks</td><td>1</td></tr><tr><td>best</td><td>1</td></tr><tr><td>2.</td><td>1</td></tr><tr><td>concurrently</td><td>1</td></tr><tr><td>Spark</td><td>2</td></tr><tr><td>4</td><td>1</td></tr><tr><td>a</td><td>1</td></tr><tr><td>faster</td><td>1</td></tr><tr><td></td><td>14</td></tr><tr><td>parquet</td><td>1</td></tr><tr><td>1.</td><td>1</td></tr><tr><td>efficiency</td><td>1</td></tr><tr><td>improvement:</td><td>1</td></tr><tr><td>large</td><td>1</td></tr><tr><td>processing</td><td>1</td></tr><tr><td>execute</td><td>1</td></tr><tr><td>to</td><td>4</td></tr><tr><td>multiple</td><td>2</td></tr><tr><td>storage</td><td>1</td></tr><tr><td>which</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "used",
         1
        ],
        [
         "space",
         1
        ],
        [
         "spark.config.set(\"spark.serializer\",\"kryo",
         1
        ],
        [
         "compression",
         1
        ],
        [
         "=",
         1
        ],
        [
         "spark.sparkContext.defaultParallelism",
         1
        ],
        [
         "because",
         1
        ],
        [
         "format",
         1
        ],
        [
         "process",
         1
        ],
        [
         "+snappy",
         1
        ],
        [
         "context",
         1
        ],
        [
         "more",
         1
        ],
        [
         "3.",
         1
        ],
        [
         "parralelisim:",
         1
        ],
        [
         "along",
         1
        ],
        [
         "for",
         1
        ],
        [
         "refers",
         1
        ],
        [
         "distribute",
         1
        ],
        [
         "in",
         3
        ],
        [
         "with",
         1
        ],
        [
         "terms",
         1
        ],
        [
         "nodes",
         1
        ],
        [
         "serializer\")",
         1
        ],
        [
         "allows",
         1
        ],
        [
         "is",
         3
        ],
        [
         "serialisation:",
         1
        ],
        [
         "File",
         1
        ],
        [
         "it",
         2
        ],
        [
         "data",
         2
        ],
        [
         "processing.",
         1
        ],
        [
         "doing",
         1
        ],
        [
         "It",
         1
        ],
        [
         "enabling",
         1
        ],
        [
         "spark",
         1
        ],
        [
         "while",
         1
        ],
        [
         "file",
         1
        ],
        [
         "computation",
         1
        ],
        [
         "the",
         4
        ],
        [
         "across",
         1
        ],
        [
         "format:",
         1
        ],
        [
         "snappy",
         1
        ],
        [
         "datasets",
         1
        ],
        [
         "and",
         2
        ],
        [
         "lot",
         1
        ],
        [
         "cluster,",
         1
        ],
        [
         "of",
         3
        ],
        [
         "gives",
         1
        ],
        [
         "columunar",
         1
        ],
        [
         "ability",
         1
        ],
        [
         "efficiently.",
         1
        ],
        [
         "Parallelism",
         1
        ],
        [
         "performance",
         1
        ],
        [
         "tasks",
         1
        ],
        [
         "best",
         1
        ],
        [
         "2.",
         1
        ],
        [
         "concurrently",
         1
        ],
        [
         "Spark",
         2
        ],
        [
         "4",
         1
        ],
        [
         "a",
         1
        ],
        [
         "faster",
         1
        ],
        [
         "",
         14
        ],
        [
         "parquet",
         1
        ],
        [
         "1.",
         1
        ],
        [
         "efficiency",
         1
        ],
        [
         "improvement:",
         1
        ],
        [
         "large",
         1
        ],
        [
         "processing",
         1
        ],
        [
         "execute",
         1
        ],
        [
         "to",
         4
        ],
        [
         "multiple",
         2
        ],
        [
         "storage",
         1
        ],
        [
         "which",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "split_values",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_group = df_split.groupBy(\"split_values\").agg(count(\"*\").alias(\"count\"))\n",
    "display(df_group)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1690521703935455,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "count the number of words in text file using map reduce",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
