{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a05b415b-dab9-44b9-80e7-f218f0841278",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example-app\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd0c54e5-f65c-4075-bf15-af9373b0663c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1684025d-5a07-4cfc-b242-63de212621df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = IntegerType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7e415b5-d659-49fe-a567-34de30b04aea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3377959906186315>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m rdd\u001B[38;5;241m=\u001B[39msc\u001B[38;5;241m.\u001B[39mparallelize(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m9\u001B[39m))\n",
       "\u001B[1;32m      2\u001B[0m rdd\u001B[38;5;241m.\u001B[39mcollect()\n",
       "\u001B[0;32m----> 3\u001B[0m df\u001B[38;5;241m=\u001B[39m rdd\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x: (x,))\u001B[38;5;241m.\u001B[39mtoDF([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m'\u001B[39m],schema)\n",
       "\u001B[1;32m      4\u001B[0m df\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:113\u001B[0m, in \u001B[0;36m_monkey_patch_RDD.<locals>.toDF\u001B[0;34m(self, schema, sampleRatio)\u001B[0m\n",
       "\u001B[1;32m     78\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n",
       "\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtoDF\u001B[39m(\u001B[38;5;28mself\u001B[39m, schema\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sampleRatio\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n",
       "\u001B[1;32m     80\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m     81\u001B[0m \u001B[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001B[39;00m\n",
       "\u001B[1;32m     82\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    111\u001B[0m \u001B[38;5;124;03m    +---+\u001B[39;00m\n",
       "\u001B[1;32m    112\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 113\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msampleRatio\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1216\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n",
       "\u001B[1;32m   1212\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n",
       "\u001B[1;32m   1213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n",
       "\u001B[1;32m   1214\u001B[0m         data, schema, samplingRatio, verifySchema\n",
       "\u001B[1;32m   1215\u001B[0m     )\n",
       "\u001B[0;32m-> 1216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1217\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n",
       "\u001B[1;32m   1218\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1264\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1263\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, RDD):\n",
       "\u001B[0;32m-> 1264\u001B[0m         rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromRDD\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1265\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1266\u001B[0m         rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:834\u001B[0m, in \u001B[0;36mSparkSession._createFromRDD\u001B[0;34m(self, rdd, schema, samplingRatio)\u001B[0m\n",
       "\u001B[1;32m    830\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    831\u001B[0m \u001B[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001B[39;00m\n",
       "\u001B[1;32m    832\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    833\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n",
       "\u001B[0;32m--> 834\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inferSchema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrdd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    835\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n",
       "\u001B[1;32m    836\u001B[0m     tupled_rdd \u001B[38;5;241m=\u001B[39m rdd\u001B[38;5;241m.\u001B[39mmap(converter)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:812\u001B[0m, in \u001B[0;36mSparkSession._inferSchema\u001B[0;34m(self, rdd, samplingRatio, names)\u001B[0m\n",
       "\u001B[1;32m    807\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m    808\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSome of types cannot be determined by the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    809\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfirst 100 rows, please try again with sampling\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    810\u001B[0m             )\n",
       "\u001B[1;32m    811\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 812\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43msamplingRatio\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m<\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.99\u001B[39;49m:\n",
       "\u001B[1;32m    813\u001B[0m         rdd \u001B[38;5;241m=\u001B[39m rdd\u001B[38;5;241m.\u001B[39msample(\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28mfloat\u001B[39m(samplingRatio))\n",
       "\u001B[1;32m    814\u001B[0m     schema \u001B[38;5;241m=\u001B[39m rdd\u001B[38;5;241m.\u001B[39mmap(\n",
       "\u001B[1;32m    815\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m row: _infer_schema(\n",
       "\u001B[1;32m    816\u001B[0m             row,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    820\u001B[0m         )\n",
       "\u001B[1;32m    821\u001B[0m     )\u001B[38;5;241m.\u001B[39mreduce(_merge_type)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: '<' not supported between instances of 'IntegerType' and 'float'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-3377959906186315>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m rdd\u001B[38;5;241m=\u001B[39msc\u001B[38;5;241m.\u001B[39mparallelize(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m9\u001B[39m))\n\u001B[1;32m      2\u001B[0m rdd\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[0;32m----> 3\u001B[0m df\u001B[38;5;241m=\u001B[39m rdd\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x: (x,))\u001B[38;5;241m.\u001B[39mtoDF([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m'\u001B[39m],schema)\n\u001B[1;32m      4\u001B[0m df\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:113\u001B[0m, in \u001B[0;36m_monkey_patch_RDD.<locals>.toDF\u001B[0;34m(self, schema, sampleRatio)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtoDF\u001B[39m(\u001B[38;5;28mself\u001B[39m, schema\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sampleRatio\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001B[39;00m\n\u001B[1;32m     82\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;124;03m    +---+\u001B[39;00m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 113\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msampleRatio\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1216\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m   1212\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[1;32m   1213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[1;32m   1214\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[1;32m   1215\u001B[0m     )\n\u001B[0;32m-> 1216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1217\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1218\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1264\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1263\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, RDD):\n\u001B[0;32m-> 1264\u001B[0m         rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromRDD\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1265\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1266\u001B[0m         rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:834\u001B[0m, in \u001B[0;36mSparkSession._createFromRDD\u001B[0;34m(self, rdd, schema, samplingRatio)\u001B[0m\n\u001B[1;32m    830\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    831\u001B[0m \u001B[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001B[39;00m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    833\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m--> 834\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inferSchema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrdd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    835\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n\u001B[1;32m    836\u001B[0m     tupled_rdd \u001B[38;5;241m=\u001B[39m rdd\u001B[38;5;241m.\u001B[39mmap(converter)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:812\u001B[0m, in \u001B[0;36mSparkSession._inferSchema\u001B[0;34m(self, rdd, samplingRatio, names)\u001B[0m\n\u001B[1;32m    807\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    808\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSome of types cannot be determined by the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    809\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfirst 100 rows, please try again with sampling\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    810\u001B[0m             )\n\u001B[1;32m    811\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 812\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43msamplingRatio\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m<\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.99\u001B[39;49m:\n\u001B[1;32m    813\u001B[0m         rdd \u001B[38;5;241m=\u001B[39m rdd\u001B[38;5;241m.\u001B[39msample(\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28mfloat\u001B[39m(samplingRatio))\n\u001B[1;32m    814\u001B[0m     schema \u001B[38;5;241m=\u001B[39m rdd\u001B[38;5;241m.\u001B[39mmap(\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m row: _infer_schema(\n\u001B[1;32m    816\u001B[0m             row,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    820\u001B[0m         )\n\u001B[1;32m    821\u001B[0m     )\u001B[38;5;241m.\u001B[39mreduce(_merge_type)\n\n\u001B[0;31mTypeError\u001B[0m: '<' not supported between instances of 'IntegerType' and 'float'",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: '<' not supported between instances of 'IntegerType' and 'float'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd=sc.parallelize(range(1,9))\n",
    "rdd.collect()\n",
    "df= rdd.map(lambda x: (x,)).toDf(['value'],schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c39346de-1f78-4739-b1fe-4b7c7575bed9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[28]: [1,\n 2,\n 3,\n 4,\n 5,\n 6,\n 7,\n 8,\n 9,\n 10,\n 11,\n 12,\n 13,\n 14,\n 15,\n 16,\n 17,\n 18,\n 19,\n 20,\n 21,\n 22,\n 23,\n 24,\n 25,\n 26,\n 27,\n 28,\n 29,\n 30,\n 31,\n 32,\n 33,\n 34,\n 35,\n 36,\n 37,\n 38,\n 39,\n 40,\n 41,\n 42,\n 43,\n 44,\n 45,\n 46,\n 47,\n 48,\n 49,\n 50,\n 51,\n 52,\n 53,\n 54,\n 55,\n 56,\n 57,\n 58,\n 59,\n 60,\n 61,\n 62,\n 63,\n 64,\n 65,\n 66,\n 67,\n 68,\n 69,\n 70,\n 71,\n 72,\n 73,\n 74,\n 75,\n 76,\n 77,\n 78,\n 79,\n 80,\n 81,\n 82,\n 83,\n 84,\n 85,\n 86,\n 87,\n 88,\n 89,\n 90,\n 91,\n 92,\n 93,\n 94,\n 95,\n 96,\n 97,\n 98,\n 99]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "# Create a SparkSession (the entry point to Spark SQL)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example-app\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create an RDD\n",
    "rdd = spark.sparkContext.parallelize(range(1, 100))\n",
    "rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfedb528-af4e-4f7e-9fc8-c1214ef8530d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|value|\n+-----+\n|    1|\n|    2|\n|    3|\n|    4|\n|    5|\n|    6|\n|    7|\n|    8|\n|    9|\n|   10|\n|   11|\n|   12|\n|   13|\n|   14|\n|   15|\n|   16|\n|   17|\n|   18|\n|   19|\n|   20|\n+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Define the schema explicitly as IntegerType\n",
    "schema = StructType([StructField(\"value\", IntegerType(), True)])\n",
    "\n",
    "# Convert RDD to DataFrame with the defined schema\n",
    "df = spark.createDataFrame(rdd.flatMap(lambda x: [(x,)]), schema)\n",
    "# Convert RDD to DataFrame with the defined schema using map\n",
    "#df = spark.createDataFrame(rdd.Map(lambda x: (x,)), schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "how to create a sequence in spark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
