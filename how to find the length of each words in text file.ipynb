{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d4c3e5f-50da-4182-8b2d-854101afb324",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.master(\"local\").appName(\"word_count\").getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12856ed9-3dbe-4909-a636-3a2904b702db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd=sc.textFile(\"/FileStore/tables/spark_performance_improvement.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fbd60bb-e63d-45c1-ba7a-fcd8187c5690",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: [('spark performance improvement:', 30),\n ('    parquet +snappy is the best because it is columunar file format which gives lot of efficiency in terms of storage space and while doing processing and along with the snappy compression it is used for large datasets',\n  218),\n ('2. parralelisim:', 16),\n ('Parallelism in the context of Spark refers to the ability to execute multiple tasks concurrently to process data more efficiently. It allows Spark to distribute computation across multiple nodes in a cluster',\n  207),\n (' enabling faster data processing.', 33),\n ('    spark.sparkContext.defaultParallelism = 4', 45),\n ('3. serialisation:', 17),\n ('spark.config.set(\"spark.serializer\"', 35),\n ('\"kryo serializer\")', 18)]"
     ]
    }
   ],
   "source": [
    "rdd_flatmap=rdd.flatMap(lambda x: x.split(','))\\\n",
    "               .map(lambda x: (x,len(x)))\\\n",
    "               .filter(lambda x: x[1]>15)\n",
    "rdd_flatmap.collect()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "how to find the length of each words in text file",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
