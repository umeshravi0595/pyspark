{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b0df224-dd54-4ce0-83a9-747b66e05c8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 105 bytes.\nOut[1]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.put(\"/FileStore/tables/dynamicolumns.csv\",\n",
    "\"\"\"\n",
    "id,name,loc,age,sex\n",
    "1,ravi,bangalore,33,m\n",
    "2,raj,chennai\n",
    "3,mohan\n",
    "4,prasad,hyderabad,35\n",
    "5,sridhar,chennai\n",
    "\"\"\",True)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e41e346-66f0-49cb-8a11-778dc6dc0ec0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>loc</th><th>age</th><th>sex</th></tr></thead><tbody><tr><td>1</td><td>ravi</td><td>bangalore</td><td>33</td><td>m</td></tr><tr><td>2</td><td>raj</td><td>chennai</td><td>null</td><td>null</td></tr><tr><td>3</td><td>mohan</td><td>null</td><td>null</td><td>null</td></tr><tr><td>4</td><td>prasad</td><td>hyderabad</td><td>35</td><td>null</td></tr><tr><td>5</td><td>sridhar</td><td>chennai</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "ravi",
         "bangalore",
         "33",
         "m"
        ],
        [
         "2",
         "raj",
         "chennai",
         null,
         null
        ],
        [
         "3",
         "mohan",
         null,
         null,
         null
        ],
        [
         "4",
         "prasad",
         "hyderabad",
         "35",
         null
        ],
        [
         "5",
         "sridhar",
         "chennai",
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "loc",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sex",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.csv(\"/FileStore/tables/dynamicolumns.csv\",header=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e22a6ad3-82d0-4f2f-acd6-f7548111916d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 96 bytes.\nOut[3]: True"
     ]
    }
   ],
   "source": [
    "#same data without headers\n",
    "dbutils.fs.put(\"/FileStore/tables/dynamicolumns_withoutheader.csv\",\n",
    "\"\"\"1,ravi,bangalore\n",
    "2,raj,chennai,33,m\n",
    "3,mohan\n",
    "4,prasad,hyderabad,35,m,787878987\n",
    "5,sridhar,chennai\n",
    "\"\"\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c873b179-0483-41ae-8ee3-73baa43ac435",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th></tr></thead><tbody><tr><td>1</td><td>ravi</td><td>bangalore</td></tr><tr><td>2</td><td>raj</td><td>chennai</td></tr><tr><td>3</td><td>mohan</td><td>null</td></tr><tr><td>4</td><td>prasad</td><td>hyderabad</td></tr><tr><td>5</td><td>sridhar</td><td>chennai</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "ravi",
         "bangalore"
        ],
        [
         "2",
         "raj",
         "chennai"
        ],
        [
         "3",
         "mohan",
         null
        ],
        [
         "4",
         "prasad",
         "hyderabad"
        ],
        [
         "5",
         "sridhar",
         "chennai"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_c0",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c2",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#same data without headers it will now read the first two columns since it have variable columns for rows\n",
    "df1 = spark.read.csv(\"/FileStore/tables/dynamicolumns_withoutheader.csv\")\n",
    "display(df1)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccaf25a3-ae3c-453b-b51d-152b34613812",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|               value|\n+--------------------+\n|    1,ravi,bangalore|\n|  2,raj,chennai,33,m|\n|             3,mohan|\n|4,prasad,hyderaba...|\n|   5,sridhar,chennai|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create Dataframe reading csv file using spark.read.text api\n",
    "df1 = spark.read.text(\"/FileStore/tables/dynamicolumns_withoutheader.csv\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4f2c6f7-4275-4b44-b2ef-bd59958f58a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>splitted_col</th></tr></thead><tbody><tr><td>List(1, ravi, bangalore)</td></tr><tr><td>List(2, raj, chennai, 33, m)</td></tr><tr><td>List(3, mohan)</td></tr><tr><td>List(4, prasad, hyderabad, 35, m, 787878987)</td></tr><tr><td>List(5, sridhar, chennai)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          "1",
          "ravi",
          "bangalore"
         ]
        ],
        [
         [
          "2",
          "raj",
          "chennai",
          "33",
          "m"
         ]
        ],
        [
         [
          "3",
          "mohan"
         ]
        ],
        [
         [
          "4",
          "prasad",
          "hyderabad",
          "35",
          "m",
          "787878987"
         ]
        ],
        [
         [
          "5",
          "sridhar",
          "chennai"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "splitted_col",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":false}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import *\n",
    "# Split text data using split function with comma delimieter\n",
    "df3 =df1.select(split(\"value\",\",\").alias(\"splitted_col\"))\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2735b972-1147-4f5a-979c-21f589761f24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+------------------+\n|splitted_col                            |size(splitted_col)|\n+----------------------------------------+------------------+\n|[1, ravi, bangalore]                    |3                 |\n|[2, raj, chennai, 33, m]                |5                 |\n|[3, mohan]                              |2                 |\n|[4, prasad, hyderabad, 35, m, 787878987]|6                 |\n|[5, sridhar, chennai]                   |3                 |\n+----------------------------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get Length of each row using size function then find max length of row for generating no of columns dynamically\n",
    "df3.select('splitted_col',size('splitted_col')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7bdb703-ec91-4e55-ae95-c0e923dfb60e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range (splitable_df.select(max(size(\"Splitable_col\"))).collect([0][0])) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74151fcf-3f67-42f2-a850-c0b4b08da0a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[27]: 6"
     ]
    }
   ],
   "source": [
    "#df3.select(max(size('splitted_col'))).first()[0]\n",
    "# Verify no of columns is going to generate this from data.\n",
    "df3.select(max(size('splitted_col'))).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffbf8392-0b1b-4a1b-a462-9b6a0ee7dd63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-31332495302255>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(df3\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;28mmax\u001B[39m(size(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msplitted_col\u001B[39m\u001B[38;5;124m'\u001B[39m)))\u001B[38;5;241m.\u001B[39mcollect()[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]):\n",
       "\u001B[1;32m      2\u001B[0m     \u001B[38;5;66;03m# Dynamically Add Columns using WithColumn \u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m     df3\u001B[38;5;241m=\u001B[39mdf3\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(i),df3[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplitted_col\u001B[39m\u001B[38;5;124m\"\u001B[39m][i])\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Drop splitted_Col which is not required after splitting into individual columns\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   2980\u001B[0m \n",
       "\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n",
       "\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `splitted_col` cannot be resolved. Did you mean one of the following? [`col0`, `col1`, `col2`, `col3`, `col4`].;\n",
       "'Aggregate [unresolvedalias(max(size('splitted_col, true)), Some(org.apache.spark.sql.Column$$Lambda$6999/1982928348@79011479))]\n",
       "+- Project [col0#1302, col1#1310, col2#1318, col3#1326, col4#1334, col5#1342]\n",
       "   +- Project [splitted_col#946, col0#1302, col1#1310, col2#1318, col3#1326, col4#1334, splitted_col#946[5] AS col5#1342]\n",
       "      +- Project [splitted_col#946, col0#1302, col1#1310, col2#1318, col3#1326, splitted_col#946[4] AS col4#1334, col5#1234]\n",
       "         +- Project [splitted_col#946, col0#1302, col1#1310, col2#1318, splitted_col#946[3] AS col3#1326, col4#1206, col5#1234]\n",
       "            +- Project [splitted_col#946, col0#1302, col1#1310, splitted_col#946[2] AS col2#1318, col3#1178, col4#1206, col5#1234]\n",
       "               +- Project [splitted_col#946, col0#1302, splitted_col#946[1] AS col1#1310, col2#1150, col3#1178, col4#1206, col5#1234]\n",
       "                  +- Project [splitted_col#946, splitted_col#946[0] AS col0#1302, col1#1122, col2#1150, col3#1178, col4#1206, col5#1234]\n",
       "                     +- Project [splitted_col#946, col0#1094, col1#1122, col2#1150, col3#1178, col4#1206, splitted_col#946[5] AS col5#1234]\n",
       "                        +- Project [splitted_col#946, col0#1094, col1#1122, col2#1150, col3#1178, splitted_col#946[4] AS col4#1206, col5#1055]\n",
       "                           +- Project [splitted_col#946, col0#1094, col1#1122, col2#1150, splitted_col#946[3] AS col3#1178, col4#1031, col5#1055]\n",
       "                              +- Project [splitted_col#946, col0#1094, col1#1122, splitted_col#946[2] AS col2#1150, col3#1011, col4#1031, col5#1055]\n",
       "                                 +- Project [splitted_col#946, col0#1094, splitted_col#946[1] AS col1#1122, col2#995, col3#1011, col4#1031, col5#1055]\n",
       "                                    +- Project [splitted_col#946, splitted_col#946[0] AS col0#1094, col1#983, col2#995, col3#1011, col4#1031, col5#1055]\n",
       "                                       +- Project [splitted_col#946, col0#975, col1#983, col2#995, col3#1011, col4#1031, splitted_col#946[5] AS col5#1055]\n",
       "                                          +- Project [splitted_col#946, col0#975, col1#983, col2#995, col3#1011, splitted_col#946[4] AS col4#1031]\n",
       "                                             +- Project [splitted_col#946, col0#975, col1#983, col2#995, splitted_col#946[3] AS col3#1011]\n",
       "                                                +- Project [splitted_col#946, col0#975, col1#983, splitted_col#946[2] AS col2#995]\n",
       "                                                   +- Project [splitted_col#946, col0#975, splitted_col#946[1] AS col1#983]\n",
       "                                                      +- Project [splitted_col#946, splitted_col#946[0] AS col0#975]\n",
       "                                                         +- Project [split(value#937, ,, -1) AS splitted_col#946]\n",
       "                                                            +- Relation [value#937] text\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-31332495302255>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(df3\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;28mmax\u001B[39m(size(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msplitted_col\u001B[39m\u001B[38;5;124m'\u001B[39m)))\u001B[38;5;241m.\u001B[39mcollect()[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;66;03m# Dynamically Add Columns using WithColumn \u001B[39;00m\n\u001B[1;32m      3\u001B[0m     df3\u001B[38;5;241m=\u001B[39mdf3\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(i),df3[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplitted_col\u001B[39m\u001B[38;5;124m\"\u001B[39m][i])\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Drop splitted_Col which is not required after splitting into individual columns\u001B[39;00m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2980\u001B[0m \n\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `splitted_col` cannot be resolved. Did you mean one of the following? [`col0`, `col1`, `col2`, `col3`, `col4`].;\n'Aggregate [unresolvedalias(max(size('splitted_col, true)), Some(org.apache.spark.sql.Column$$Lambda$6999/1982928348@79011479))]\n+- Project [col0#1302, col1#1310, col2#1318, col3#1326, col4#1334, col5#1342]\n   +- Project [splitted_col#946, col0#1302, col1#1310, col2#1318, col3#1326, col4#1334, splitted_col#946[5] AS col5#1342]\n      +- Project [splitted_col#946, col0#1302, col1#1310, col2#1318, col3#1326, splitted_col#946[4] AS col4#1334, col5#1234]\n         +- Project [splitted_col#946, col0#1302, col1#1310, col2#1318, splitted_col#946[3] AS col3#1326, col4#1206, col5#1234]\n            +- Project [splitted_col#946, col0#1302, col1#1310, splitted_col#946[2] AS col2#1318, col3#1178, col4#1206, col5#1234]\n               +- Project [splitted_col#946, col0#1302, splitted_col#946[1] AS col1#1310, col2#1150, col3#1178, col4#1206, col5#1234]\n                  +- Project [splitted_col#946, splitted_col#946[0] AS col0#1302, col1#1122, col2#1150, col3#1178, col4#1206, col5#1234]\n                     +- Project [splitted_col#946, col0#1094, col1#1122, col2#1150, col3#1178, col4#1206, splitted_col#946[5] AS col5#1234]\n                        +- Project [splitted_col#946, col0#1094, col1#1122, col2#1150, col3#1178, splitted_col#946[4] AS col4#1206, col5#1055]\n                           +- Project [splitted_col#946, col0#1094, col1#1122, col2#1150, splitted_col#946[3] AS col3#1178, col4#1031, col5#1055]\n                              +- Project [splitted_col#946, col0#1094, col1#1122, splitted_col#946[2] AS col2#1150, col3#1011, col4#1031, col5#1055]\n                                 +- Project [splitted_col#946, col0#1094, splitted_col#946[1] AS col1#1122, col2#995, col3#1011, col4#1031, col5#1055]\n                                    +- Project [splitted_col#946, splitted_col#946[0] AS col0#1094, col1#983, col2#995, col3#1011, col4#1031, col5#1055]\n                                       +- Project [splitted_col#946, col0#975, col1#983, col2#995, col3#1011, col4#1031, splitted_col#946[5] AS col5#1055]\n                                          +- Project [splitted_col#946, col0#975, col1#983, col2#995, col3#1011, splitted_col#946[4] AS col4#1031]\n                                             +- Project [splitted_col#946, col0#975, col1#983, col2#995, splitted_col#946[3] AS col3#1011]\n                                                +- Project [splitted_col#946, col0#975, col1#983, splitted_col#946[2] AS col2#995]\n                                                   +- Project [splitted_col#946, col0#975, splitted_col#946[1] AS col1#983]\n                                                      +- Project [splitted_col#946, splitted_col#946[0] AS col0#975]\n                                                         +- Project [split(value#937, ,, -1) AS splitted_col#946]\n                                                            +- Relation [value#937] text\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `splitted_col` cannot be resolved. Did you mean one of the following? [`col0`, `col1`, `col2`, `col3`, `col4`].;\n'Aggregate [unresolvedalias(max(size('splitted_col, true)), Some(org.apache.spark.sql.Column$$Lambda$6999/1982928348@79011479))]\n+- Project [col0#1302, col1#1310, col2#1318, col3#1326, col4#1334, col5#1342]\n   +- Project [splitted_col#946, col0#1302, col1#1310, col2#1318, col3#1326, col4#1334, splitted_col#946[5] AS col5#1342]\n      +- Project [splitted_col#946, col0#1302, col1#1310, col2#1318, col3#1326, splitted_col#946[4] AS col4#1334, col5#1234]\n         +- Project [splitted_col#946, col0#1302, col1#1310, col2#1318, splitted_col#946[3] AS col3#1326, col4#1206, col5#1234]\n            +- Project [splitted_col#946, col0#1302, col1#1310, splitted_col#946[2] AS col2#1318, col3#1178, col4#1206, col5#1234]\n               +- Project [splitted_col#946, col0#1302, splitted_col#946[1] AS col1#1310, col2#1150, col3#1178, col4#1206, col5#1234]\n                  +- Project [splitted_col#946, splitted_col#946[0] AS col0#1302, col1#1122, col2#1150, col3#1178, col4#1206, col5#1234]\n                     +- Project [splitted_col#946, col0#1094, col1#1122, col2#1150, col3#1178, col4#1206, splitted_col#946[5] AS col5#1234]\n                        +- Project [splitted_col#946, col0#1094, col1#1122, col2#1150, col3#1178, splitted_col#946[4] AS col4#1206, col5#1055]\n                           +- Project [splitted_col#946, col0#1094, col1#1122, col2#1150, splitted_col#946[3] AS col3#1178, col4#1031, col5#1055]\n                              +- Project [splitted_col#946, col0#1094, col1#1122, splitted_col#946[2] AS col2#1150, col3#1011, col4#1031, col5#1055]\n                                 +- Project [splitted_col#946, col0#1094, splitted_col#946[1] AS col1#1122, col2#995, col3#1011, col4#1031, col5#1055]\n                                    +- Project [splitted_col#946, splitted_col#946[0] AS col0#1094, col1#983, col2#995, col3#1011, col4#1031, col5#1055]\n                                       +- Project [splitted_col#946, col0#975, col1#983, col2#995, col3#1011, col4#1031, splitted_col#946[5] AS col5#1055]\n                                          +- Project [splitted_col#946, col0#975, col1#983, col2#995, col3#1011, splitted_col#946[4] AS col4#1031]\n                                             +- Project [splitted_col#946, col0#975, col1#983, col2#995, splitted_col#946[3] AS col3#1011]\n                                                +- Project [splitted_col#946, col0#975, col1#983, splitted_col#946[2] AS col2#995]\n                                                   +- Project [splitted_col#946, col0#975, splitted_col#946[1] AS col1#983]\n                                                      +- Project [splitted_col#946, splitted_col#946[0] AS col0#975]\n                                                         +- Project [split(value#937, ,, -1) AS splitted_col#946]\n                                                            +- Relation [value#937] text\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(df3.select(max(size('splitted_col'))).collect()[0][0]):\n",
    "    # Dynamically Add Columns using WithColumn \n",
    "    df3=df3.withColumn('col'+str(i),df3[\"splitted_col\"][i])\n",
    "# Drop splitted_Col which is not required after splitting into individual columns\n",
    "df3 = df3.drop(\"splitted_col\")\n",
    "\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "how to create Filedynamic columns with variable size of columns in pyspark dataframe",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
